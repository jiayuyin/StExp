应用态内存泄漏和内核内存泄漏

[get_mem.sh](./code/get_mem.sh)  
这是抓应用态进程内存分布的 ，主要是ps中不带【】的进程；  
存日志到/var/mem.log  然后在服务器使用py脚本解析日志生成xml  
[count_mem.py](./code/count_mem.py)

## SLUB分配器

确认应用进程没有泄露  就重点看slab信息
开启slub-debug，重编内核版本；
内核内存泄漏一般都是在slab上可以观察到增加：cat /proc/slabinfo，具体需要查看到底是哪个slab块在增加，可用的方法： 针对slab的分布，需要开启SLUB的debug开关，

```
CONFIG_SLUB_DEBUG=y
CONFIG_SLUB_DEBUG_ON=y
```

首先观察slabinfo  

`cat /proc/slabinfo`  

再进入目录, 比如：  

`cat /sys/kernel/slab/kmalloc-8192/alloc_calls`

这样可以观察kmalloc的调用栈信息，调用次数多的一般是怀疑点，如下开启后可以看到调用次数的函数，查看cat /sys/kernel/slab/kmalloc-8192/alloc_calls：


另外，slab增加也不一定是内存泄漏。  
由于slab的机制，在某段程序需要频繁的申请内存的话，slab会预分配一段比较大的内存出来，这里就会表现为slab增加；  
还有，没用的和用的在一个Block上，没法释放对应的Block回归Linux可用内存，这里也会显示slab增加。
泄漏的话。可以观察到slabinfo中的某项active-obj是应该是持续增大的

```
/proc/slabinfo | grep "slub_test"
# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail>
slub_test            512            512        8            512         1        : tunables     0         0           0          : slabdata      1             1          0


active-obj： 当前的objects个数
num-obj: 就是代表objects的最大个数
pagesperslab: 意思是每一个slab需要几个page,可以看到名为slub_test的slab需要一个page，也就是4K。其实就是从buddy拿了一个order为0的一个页
```

## kmemleak

### 环境支持和基本操作

需要编译选项支持  
```
CONFIG_DEBUG_KMEMLEAK=y
CONFIG_DEBUG_KMEMLEAK_EARLY_LOG_SIZE=40000
CONFIG_DEBUG_FS=y                   #debug文件系统支持
CONFIG_DEBUG_KMEMLEAK_DEFAULT_OFF=n #低版本的Linux内核此选项可能不存在
```

进入系统，需要先挂载debugfs
```
mount -t debugfs nodev /sys/kernel/debug/
```
kmemleak文件路径：/sys/kernel/debug/kmemleak

手动执行内存扫描命令：
```
echo scan > /sys/kernel/debug/kmemleak
```

显示最新一次内存扫描内容命令：
```
cat /sys/kernel/debug/kmemleak
```

清除当前扫描结果
```
echo clear > /sys/kernel/debug/kmemleak
```

### 实际使用

[memory.sh](./code/memory.sh)  

使用脚本如上挂机抓取日志，主要抓的就是meminfo 和kmemleak记录的信息，十分钟抓一次

先确认slab确实有持续增加，如果怀疑有内存泄漏，再分析kmemleak的log

通过长时间挂测累计出的日志，如果同一内存泄露的对象持续累积并且没有释放，则判定其发生了内存泄露

kememleak是可能产生误报的，不过对于每次抓取都有的增加的一些函数就是重点怀疑对象

如下分析，在对日志的分析中，每十分钟pci_rx_dma_done_handle这个函数都有增加，挂测12小时，从开始挂测的200多次，到最后1700多次。

```
unreferenced object 0xffffff80199a8000 (size 8192):
  comm "softirq", pid 0, jiffies 4295147135 (age 42936.600s)
  hex dump (first 32 bytes):
    00 a0 9a 19 80 ff ff ff ff ff ff ff ff ff ff ff  ................
    ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff  ................
  backtrace:
    [<00000000e2b73281>] __kmalloc_track_caller+0x134/0x280
    [<000000005ab31168>] __alloc_skb+0xac/0x250
    [<00000000ba237e58>] skbmgr_alloc_skb4k+0x54/0x1b0
    [<00000000f9678b75>] hostadpt_rx_handler+0x678/0x7e0 [hostadpt]
    [<0000000032e42415>] pci_rx_dma_done_handle+0x1b8/0x590 [mt7916_ap]
    [<00000000fb2bcc5a>] pci_rx_data_done_func+0xd8/0x3a0 [mt7916_ap]
    [<000000003efec269>] tasklet_action_common.constprop.0+0x124/0x150
    [<0000000076794427>] tasklet_hi_action+0x24/0x30
    [<00000000a783db0f>] __do_softirq+0x124/0x27c
    [<00000000901de48f>] irq_exit+0x98/0xe0
    [<000000000fa54b0d>] __handle_domain_irq+0x74/0xd0
    [<000000006019322a>] gic_handle_irq+0x8c/0x190
    [<00000000de6a3209>] el1_irq+0xb8/0x140
    [<000000006c4a4176>] arch_cpu_idle+0x10/0x20
    [<000000001e9c24a0>] cpu_startup_entry+0x24/0x70
    [<0000000035ec80fa>] rest_init+0xb0/0xbc
    
comm "softirq"说明了是哪个模块
pid 0代表了进程号
backtrack指的是反向追踪路径，具体到函数的调用。在以上例子中内核泄露的函数为pci_rx_dma_done_handle

```
